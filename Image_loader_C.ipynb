{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import freeze_support\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "num_of_workers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "List=[]\n",
    "with open(\"./train_labels.csv\",\"r\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    #這裡不需要readlines\n",
    "    for line in reader:\n",
    "        List.append(line)\n",
    "for i in range(len(List)):\n",
    "    if i > 0:\n",
    "        List[i][0]=List[i][0]+'.png'\n",
    "data = pd.DataFrame(List[:47501],columns =['ID','Class'])\n",
    "data.drop([0],inplace=True)\n",
    "#data = pd.read_csv(\"./train_labels.csv\")\n",
    "labels = data.groupby(\"Class\")\n",
    "np.random.seed(10)\n",
    "for i, Class in enumerate(data.Class.unique()):\n",
    "    randomList = list(range(len(labels.get_group(Class))))\n",
    "    np.random.shuffle(randomList)\n",
    "    trainPart = labels.get_group(Class).iloc[randomList][:int(len(randomList)*0.8)]\n",
    "    testPart = labels.get_group(Class).iloc[randomList][int(len(randomList)*0.8):]\n",
    "    if (i == 0):\n",
    "        trainAll = trainPart\n",
    "        testAll = testPart\n",
    "    else:\n",
    "        trainAll = pd.concat([trainAll, trainPart], axis=0)\n",
    "        testAll = pd.concat([testAll, testPart], axis=0)\n",
    "trainAll.to_csv(\"./train.csv\",index=False)\n",
    "testAll.to_csv(\"./valid.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_dataset():\n",
    "    def __init__(self, csvFile, rootPath, transform):\n",
    "        self.df = pd.read_csv(csvFile)\n",
    "        self.rootPath = rootPath\n",
    "        self.xTrain = self.df['ID']\n",
    "        self.yTrain = self.df['Class']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.rootPath, self.xTrain[index]))\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.yTrain[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xTrain.index)\n",
    "    \n",
    "    def __class__(self):\n",
    "        return sorted(self.df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 224\n",
    "dataTransformsTrain = transforms.Compose([\n",
    "     transforms.RandomResizedCrop(inputSize),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "dataTransformsTest = transforms.Compose([\n",
    "     transforms.Resize(inputSize),\n",
    "     transforms.CenterCrop(inputSize),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "trainDatasets = image_dataset(\"./train.csv\", \"./train\", dataTransformsTrain)\n",
    "testDatasets = image_dataset(\"./valid.csv\", \"./train\", dataTransformsTest)\n",
    "train_loader = torch.utils.data.DataLoader(trainDatasets, batch_size=32, shuffle=True, num_workers=1)\n",
    "test_loader= torch.utils.data.DataLoader(testDatasets, batch_size=32, shuffle=False)\n",
    "\n",
    "# model = VGG16_pretrained_model(numClasses=7, featureExtract=True, usePretrained=True).to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer and loss function\n",
    "#model = CNNet(num_classes)\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "fc_features = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_features, 10)\n",
    "\n",
    "# if cuda is available, move the model to the GPU\n",
    "\n",
    "model.cuda()\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(epoch):\n",
    "    torch.save(model.state_dict(), f\"{epoch}.model\")\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    count=0\n",
    "    for x, label in test_loader:\n",
    "        x = x.to(device)\n",
    "        label = label.to(device, dtype=torch.long)\n",
    "        output = model(x)\n",
    "\n",
    "        # Predict classes using images from the test set\n",
    "        _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "        test_acc += torch.sum(prediction == label).float()\n",
    "\n",
    "    # Compute the average acc and loss over all 10000 test images\n",
    "    test_acc = test_acc /  10000* 100\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epoch):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        count=0\n",
    "        for x, label in train_loader:\n",
    "            count+=1\n",
    "            if (count%1000==0):\n",
    "                print(count)\n",
    "            x = x.to(device)\n",
    "            label = label.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #train_loss += loss.cpu().data[0] * images.size(0)\n",
    "            train_loss += loss.item()\n",
    "            _, prediction = torch.max(output.data, 1)\n",
    "           \n",
    "\n",
    "            train_acc += torch.sum(prediction == label).float()\n",
    "\n",
    "        # Call the learning rate adjustment function\n",
    "        #adjust_learning_rate(epoch)\n",
    "\n",
    "        # Compute the average acc and loss over all 50000 training images\n",
    "        train_acc = train_acc / 40000 * 100\n",
    "        train_loss = train_loss / 80000\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_acc = test()\n",
    "\n",
    "        # Save the model if the test acc is greater than our current best\n",
    "        if train_acc > best_acc:\n",
    "            save_models(epoch)\n",
    "            best_acc = train_acc\n",
    "\n",
    "        # Print the metrics\n",
    "        print(f\"Epoch {epoch + 1}, Train Accuracy: {train_acc} , TrainLoss: {train_loss} , Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 1, Train Accuracy: 68.61249542236328 , TrainLoss: 0.011864262460172177 , Test Accuracy: 84.87999725341797\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 2, Train Accuracy: 74.17250061035156 , TrainLoss: 0.00936572665348649 , Test Accuracy: 85.8699951171875\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 3, Train Accuracy: 76.0 , TrainLoss: 0.008593115441687406 , Test Accuracy: 86.07999420166016\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 4, Train Accuracy: 77.43000030517578 , TrainLoss: 0.007955341605376452 , Test Accuracy: 86.91999816894531\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 5, Train Accuracy: 78.69499969482422 , TrainLoss: 0.007472392782755196 , Test Accuracy: 88.29999542236328\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 6, Train Accuracy: 79.12249755859375 , TrainLoss: 0.007150246860273182 , Test Accuracy: 88.72000122070312\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 7, Train Accuracy: 79.49500274658203 , TrainLoss: 0.006968290718598291 , Test Accuracy: 88.72000122070312\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 8, Train Accuracy: 79.70249938964844 , TrainLoss: 0.006882890461105853 , Test Accuracy: 88.73999786376953\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 9, Train Accuracy: 80.36750030517578 , TrainLoss: 0.006558460553362965 , Test Accuracy: 88.65999603271484\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 10, Train Accuracy: 80.89500427246094 , TrainLoss: 0.006399257441330701 , Test Accuracy: 88.7699966430664\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 11, Train Accuracy: 81.36499786376953 , TrainLoss: 0.006216084041353315 , Test Accuracy: 89.22999572753906\n",
      "1000\n",
      "Epoch 12, Train Accuracy: 81.32499694824219 , TrainLoss: 0.006138079003896564 , Test Accuracy: 89.06999969482422\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 13, Train Accuracy: 82.0875015258789 , TrainLoss: 0.005911725885607302 , Test Accuracy: 88.88999938964844\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 14, Train Accuracy: 82.22000122070312 , TrainLoss: 0.005835913013666868 , Test Accuracy: 89.44000244140625\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 15, Train Accuracy: 82.40999603271484 , TrainLoss: 0.0057794011824764315 , Test Accuracy: 89.33000183105469\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 16, Train Accuracy: 82.42499542236328 , TrainLoss: 0.005718163238465786 , Test Accuracy: 89.69000244140625\n",
      "1000\n",
      "Checkpoint saved\n",
      "Epoch 17, Train Accuracy: 82.79249572753906 , TrainLoss: 0.0055426890527829524 , Test Accuracy: 89.13999938964844\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
